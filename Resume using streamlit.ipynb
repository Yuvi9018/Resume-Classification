{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b38cbb67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Win-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Win-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import docx\n",
    "# import PyPDF2\n",
    "import io \n",
    "import nltk\n",
    "# import altair as alt\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd\n",
    "# import streamlit as st\n",
    "import numpy as np\n",
    "# loading the trained model\n",
    "#pickle_in = open('classifier.pkl', 'rb') \n",
    "#classifier = pickle.load(pickle_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4bd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import  docx2txt\n",
    "from docx import Document\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string # special operations on strings\n",
    "import spacy # language models\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "import win32com.client\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import textract\n",
    "import re #regular expression\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from spacy.matcher import Matcher\n",
    "import multiprocessing as mp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "178a3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)\n",
    "def cleaned_review(review_df):\n",
    "    review_df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\",\"[|]\"], value=[\"\",\"\",\"\"], regex=True, inplace=True)\n",
    "    review_df=review_df.drop(index=[0,1,2,3,4,5,6,7,8,9],axis=0)\n",
    "    review_df=review_df.reset_index(drop=bool)\n",
    "    return review_df\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def extract_skills(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = nlp_text\n",
    "    data = pd.read_csv('E:\\DATA SETS\\Resumes\\Skill.csv')\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    return np.unique(skillset)\n",
    "    # check for bi-grams and tri-grams\n",
    "def extract_post(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = nlp_text\n",
    "    data = pd.read_csv('E:\\DATA SETS\\Resumes\\post.csv')\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    return np.unique(skillset)\n",
    "    # check for bi-grams and tri-grams\n",
    "def lem(x):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatized = [lmtzr.lemmatize(word) for word in x]\n",
    "    return lemmatized  \n",
    "\n",
    "def stem(x):\n",
    "    ps = PorterStemmer()\n",
    "    stemming = [ps.stem(word) for word in x]\n",
    "    return stemming   \n",
    "\n",
    "def extract_designation(x):\n",
    "    y =''\n",
    "    for token in x:\n",
    "        if 'develop' in x and 'sql' in x and 'oracl' in x:\n",
    "            y ='SQL and Oracle Developer'\n",
    "        elif 'consult' in x and 'workday' in x:\n",
    "            y= ' workday consultant'\n",
    "        elif 'react' in x and 'develop' in x:\n",
    "             y ='React  Developer'\n",
    "        elif 'reactjs' in x and 'develop' in x:\n",
    "             y ='ReactJS  Developer' \n",
    "        elif 'develop' in x and 'sql' in x :\n",
    "            y ='SQL  Developer'\n",
    "        elif 'hr' in x:\n",
    "            y = 'human resource'\n",
    "        elif 'peoplesoft'in x and 'admin' in x:\n",
    "            y = 'PeopleSoft Admin'\n",
    "        elif 'peoplesoft' in x and 'consult' in x:\n",
    "            y ='PeopleSoft Consultant'\n",
    "        elif token =='softwar':\n",
    "             y ='Software Engineer'\n",
    "        elif 'develop' in x and 'sql' in x :\n",
    "            y ='SQL  Developer' \n",
    "        else:\n",
    "            y ='intern'\n",
    "    return y\n",
    "def contactDetails(Text):\n",
    "    name = ''  \n",
    "    for i in range(0,3):\n",
    "        name = \" \".join([name, Text[i]])\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3cd41a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_classifier(source_directory):\n",
    "    file_docx = []\n",
    "    file_path = []\n",
    "    for file_folder in os.listdir( source_directory):\n",
    "    #final_path=\"E:/DATA SETS/Resumes/\"+file_folder+\".docx\"\n",
    "        final_path = os.path.join( source_directory, file_folder)\n",
    "        if final_path.endswith('.docx') :\n",
    "    #for j in os.listdir(os.path.join(r'E:/DATA SETS/Resumes', file_folder)):\n",
    "        #final_folder_path = os.path.join(final_path,j)\n",
    "            file_docx.append(textract.process(final_path))\n",
    "            file_path.append(file_folder)\n",
    "    df1 = pd.DataFrame(file_docx)\n",
    "    df1['new_column'] = df1.apply(remove_punctuations)\n",
    "    df1['Text'] = df1.new_column.str.decode(\"UTF-8\") \n",
    "    cleanreviewdf1=cleaned_review(df1)\n",
    "    df1.Text.drop_duplicates()\n",
    "    cleaned_df = df1.Text.apply(clean)\n",
    "    data = pd.concat([df1,cleaned_df],axis = 1)\n",
    "    stop = stopwords.words('english')\n",
    "    data['cleaned'] = cleaned_df.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df = data['cleaned'].copy()\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    df['token'] = ''\n",
    "    j=0\n",
    "    for i in cleaned_df:\n",
    "        df.token[j]=i.split()\n",
    "        j = j+1\n",
    "        \n",
    "    f = cleaned_df[0].split()\n",
    "    \n",
    "    df['skills'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['skills'][j] = extract_skills(df.token[j])\n",
    "        j = j+1\n",
    "    \n",
    "    df['post'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['post'][j] = extract_post(df.token[j])\n",
    "        j = j+1\n",
    "    df['name'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['name'][j] = contactDetails(df.token[j])\n",
    "        j = j+1    \n",
    "    \n",
    "    resume = df.drop(columns={'cleaned','token'}, axis = 1)\n",
    "    \n",
    "    resume['lem'] = ''\n",
    "    j = 0\n",
    "    for i in resume.post:\n",
    "        resume['lem'][j] = lem(resume.post[j])\n",
    "        j = j+1\n",
    "        \n",
    "    \n",
    "    \n",
    "    resume['stem'] = ''\n",
    "    j = 0\n",
    "    for i in resume.lem:\n",
    "        resume['stem'][j] = stem(resume.lem[j])\n",
    "        j = j+1\n",
    "    \n",
    "    \n",
    "    resume['designation'] =''\n",
    "    j = 0\n",
    "    for i in resume.post:\n",
    "        resume['designation'][j] = extract_designation(resume.stem[j])\n",
    "        j = j+1\n",
    "    \n",
    "    data = resume[['name','skills','designation']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    data = pd.concat([data,d],axis=1,ignore_index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad8a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import streamlit as st\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b232a9d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5216/237426862.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5216/237426862.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#         ResumeText.loc[len(ResumeText)] = NewRow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#         #st.dataframe(ResumeText)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mResumeText\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresume_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0msqlor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mResumeText\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"designation\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"SQL and Oracle Developer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m#javares = ResumeText[java]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5216/1224122780.py\u001b[0m in \u001b[0;36mresume_classifier\u001b[1;34m(source_directory)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# this is the main function in which we define our webpage  \n",
    "def main():\n",
    "\n",
    "    st.sidebar.header(\"Resume Classification App\")\n",
    "    st.sidebar.text_input('Enter the resumes folder path')\n",
    "\n",
    "    # following lines create boxes in which user can enter data required to make prediction \n",
    "    #path ='D:/Python_Practice/P-130_Project/Resumes'\n",
    "    #path = st.sidebar.text_input('Enter the resumes folder path')\n",
    "\n",
    "    # Create an empty Data Frame ResumeText with two columns\n",
    "    ResumeText = pd.DataFrame([], columns=['Name', 'skills', 'designation','designation'])\n",
    "    path = r'E:\\DATA SETS\\Resumes'\n",
    "    # when 'Predict' is clicked, make the prediction and store it \n",
    "    #if st.button(\"Upload and Get Result\"): \n",
    "        # Search the directory path and loop through the resume documents and callthe function getText\n",
    "    #if st.button(\"Process\"):\n",
    "#     for filename in os.listdir(path):\n",
    "#         filename = os.path.join(path, filename)\n",
    "#         extText = getText(filename)\n",
    "#         tokText = tokenText(extText)\n",
    "#         role = roleApplied(tokText)\n",
    "#         Name = contactDetails(tokText)\n",
    "#         experience = expDetails(extText)\n",
    "#         skills = skillSet(tokText)\n",
    "#         NewRow = [Name,experience, skills,role]\n",
    "#         ResumeText.loc[len(ResumeText)] = NewRow\n",
    "#         #st.dataframe(ResumeText)\n",
    "    ResumeText = resume_classifier(path)\n",
    "    sqlor = (ResumeText[\"designation\"] == \"SQL and Oracle Developer\")\n",
    "        #javares = ResumeText[java]\n",
    "    workday = (ResumeText[\"designation\"] == \"workday consultant\")\n",
    "    react = (ResumeText[\"designation\"] == \"React  Developer\")\n",
    "    reactjs = (ResumeText[\"designation\"] == \"ReactJS  Developer\")\n",
    "    sql = (ResumeText[\"designation\"] == \"SQL  Developer\")\n",
    "    hr = (ResumeText[\"designation\"] == \"human resource\")\n",
    "    admin = (ResumeText[\"designation\"] == \"PeopleSoft Admin\")\n",
    "    con = (ResumeText[\"designation\"] == \"PeopleSoft Consultant\")\n",
    "    soft = (ResumeText[\"designation\"] == \"Software Engineer\")\n",
    "    intern = (ResumeText[\"designation\"] == \"intern\")\n",
    "\n",
    "\n",
    "\n",
    "    if st.sidebar.button(\"Display Numbers\"):\n",
    "        st.subheader(\"No of Resumes Received\")\n",
    "        num = pd.DataFrame(ResumeText[''].value_counts())\n",
    "        num['designation'] = num.index\n",
    "        num.set_axis(['No of Resumes', 'designation'], axis='columns', inplace=True)\n",
    "        num.reset_index(inplace=True, drop=True)\n",
    "        num = num[['designation', 'No of Resumes']]\n",
    "        st.dataframe(num)\n",
    "\n",
    "        base = alt.Chart(num).encode(theta=alt.Theta(\n",
    "           \"No of Resumes:Q\", stack=True), color=alt.Color(\"designation:N\", legend=None)\n",
    "        )\n",
    "        pie = base.mark_arc(outerRadius=120)\n",
    "        text = base.mark_text(radius=165, size=15).encode(text=\"designation:N\")\n",
    "        c = pie + text\n",
    "        st.altair_chart(c, use_container_width=True)\n",
    "\n",
    "\n",
    "#     if st.sidebar.button(\"All Resumes\"):\n",
    "#         st.subheader(\"ALL RESUMES\")\n",
    "#         exp = st.slider('Select Experience', 0.0, 20.0, 0.1)\n",
    "#         expres = ResumeText[ResumeText['Exp_years'] >= exp]\n",
    "#         st.dataframe(expres)\n",
    "\n",
    "        #st.dataframe(ResumeText)\n",
    "#     if st.sidebar.button(\"JAVA Resumes\"):\n",
    "#         st.subheader(\"JAVA RESUMES\")\n",
    "#         st.dataframe(ResumeText[java])\n",
    "#     if st.sidebar.button(\"DBMS Resumes\"):\n",
    "#         st.subheader(\"DBMS RESUMES\")\n",
    "#         st.dataframe(ResumeText[dbms])\n",
    "#     if st.sidebar.button(\"Peoplesoft Resumes\"):\n",
    "#         st.subheader(\"Peoplesoft Resumes\")\n",
    "#         st.dataframe(ResumeText[peosoft])\n",
    "#     if st.sidebar.button(\"Workday Resumes\"):\n",
    "#         st.subheader(\"Workday Resumes\")\n",
    "#         st.dataframe(ResumeText[workday])\n",
    "\n",
    "\n",
    "if __name__=='__main__': \n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc967b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79270712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
