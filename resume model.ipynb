{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703b0449",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e9079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import  docx2txt\n",
    "from docx import Document\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string # special operations on strings\n",
    "import spacy # language models\n",
    "from matplotlib.pyplot import imread\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "import win32com.client\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import f1_score,precision_score,confusion_matrix,recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a238884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import re #regular expression\n",
    "import string\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cf5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pprint\n",
    "from spacy.matcher import Matcher\n",
    "import multiprocessing as mp\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba9a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(\"[0-9\" \"]+\",\" \",text)\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    return text\n",
    "\n",
    "clean = lambda x: clean_text(x)\n",
    "def cleaned_review(review_df):\n",
    "    review_df.replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\",\"[|]\"], value=[\"\",\"\",\"\"], regex=True, inplace=True)\n",
    "    review_df=review_df.drop(index=[0,1,2,3,4,5,6,7,8,9],axis=0)\n",
    "    review_df=review_df.reset_index(drop=bool)\n",
    "    return review_df\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "\n",
    "def extract_skills(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = nlp_text\n",
    "    data = pd.read_csv('E:\\DATA SETS\\Resumes\\Skill.csv')\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    return np.unique(skillset)\n",
    "    # check for bi-grams and tri-grams\n",
    "def extract_post(nlp_text):\n",
    "    '''\n",
    "    Helper function to extract skills from spacy nlp text\n",
    "    :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "    :param noun_chunks: noun chunks extracted from nlp text\n",
    "    :return: list of skills extracted\n",
    "    '''\n",
    "    tokens = nlp_text\n",
    "    data = pd.read_csv('E:\\DATA SETS\\Resumes\\post.csv')\n",
    "    skills = list(data.columns.values)\n",
    "    skillset = []\n",
    "    # check for one-grams\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    return np.unique(skillset)\n",
    "    # check for bi-grams and tri-grams\n",
    "def lem(x):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    lemmatized = [lmtzr.lemmatize(word) for word in x]\n",
    "    return lemmatized  \n",
    "\n",
    "def stem(x):\n",
    "    ps = PorterStemmer()\n",
    "    stemming = [ps.stem(word) for word in x]\n",
    "    return stemming   \n",
    "\n",
    "def extract_designation(x):\n",
    "    y =''\n",
    "    for token in x:\n",
    "        if 'develop' in x and 'sql' in x and 'oracl' in x:\n",
    "            y ='SQL and Oracle Developer'\n",
    "        elif 'consult' in x and 'workday' in x:\n",
    "            y= ' workday consultant'\n",
    "        elif 'react' in x and 'develop' in x:\n",
    "             y ='React  Developer'\n",
    "        elif 'reactjs' in x and 'develop' in x:\n",
    "             y ='ReactJS  Developer' \n",
    "        elif 'develop' in x and 'sql' in x :\n",
    "            y ='SQL  Developer'\n",
    "        elif 'hr' in x:\n",
    "            y = 'human resource'\n",
    "        elif 'peoplesoft'in x and 'admin' in x:\n",
    "            y = 'PeopleSoft Admin'\n",
    "        elif 'peoplesoft' in x and 'consult' in x:\n",
    "            y ='PeopleSoft Consultant'\n",
    "        elif token =='softwar':\n",
    "             y ='Software Engineer'\n",
    "        elif 'develop' in x and 'sql' in x :\n",
    "            y ='SQL  Developer' \n",
    "        else:\n",
    "            y ='intern'\n",
    "    return y\n",
    "def contactDetails(Text):\n",
    "    name = ''  \n",
    "    for i in range(0,3):\n",
    "        name = \" \".join([name, Text[i]])\n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979e443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_classifier(source_directory):\n",
    "    file_docx = []\n",
    "    file_path = []\n",
    "    for file_folder in os.listdir( source_directory):\n",
    "    #final_path=\"E:/DATA SETS/Resumes/\"+file_folder+\".docx\"\n",
    "        final_path = os.path.join( source_directory, file_folder)\n",
    "        if final_path.endswith('.docx') :\n",
    "    #for j in os.listdir(os.path.join(r'E:/DATA SETS/Resumes', file_folder)):\n",
    "        #final_folder_path = os.path.join(final_path,j)\n",
    "            file_docx.append(textract.process(final_path))\n",
    "            file_path.append(file_folder)\n",
    "    df1 = pd.DataFrame(file_docx)\n",
    "    df1['new_column'] = df1.apply(remove_punctuations)\n",
    "    df1['Text'] = df1.new_column.str.decode(\"UTF-8\") \n",
    "    cleanreviewdf1=cleaned_review(df1)\n",
    "    df1.Text.drop_duplicates()\n",
    "    cleaned_df = df1.Text.apply(clean)\n",
    "    data = pd.concat([df1,cleaned_df],axis = 1)\n",
    "    stop = stopwords.words('english')\n",
    "    data['cleaned'] = cleaned_df.apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "    df = data['cleaned'].copy()\n",
    "    df = pd.DataFrame(df)\n",
    "    \n",
    "    df['token'] = ''\n",
    "    j=0\n",
    "    for i in cleaned_df:\n",
    "        df.token[j]=i.split()\n",
    "        j = j+1\n",
    "        \n",
    "    f = cleaned_df[0].split()\n",
    "    \n",
    "    df['skills'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['skills'][j] = extract_skills(df.token[j])\n",
    "        j = j+1\n",
    "    \n",
    "    df['post'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['post'][j] = extract_post(df.token[j])\n",
    "        j = j+1\n",
    "    df['name'] = ''\n",
    "    j = 0\n",
    "    for i in df.token:\n",
    "        df['name'][j] = contactDetails(df.token[j])\n",
    "        j = j+1    \n",
    "    \n",
    "    resume = df.drop(columns={'cleaned','token'}, axis = 1)\n",
    "    \n",
    "    resume['lem'] = ''\n",
    "    j = 0\n",
    "    for i in resume.post:\n",
    "        resume['lem'][j] = lem(resume.post[j])\n",
    "        j = j+1\n",
    "        \n",
    "    \n",
    "    \n",
    "    resume['stem'] = ''\n",
    "    j = 0\n",
    "    for i in resume.lem:\n",
    "        resume['stem'][j] = stem(resume.lem[j])\n",
    "        j = j+1\n",
    "    \n",
    "    \n",
    "    resume['designation'] =''\n",
    "    j = 0\n",
    "    for i in resume.post:\n",
    "        resume['designation'][j] = extract_designation(resume.stem[j])\n",
    "        j = j+1\n",
    "    \n",
    "    data = resume[['name','skills','designation']]\n",
    "    \n",
    "    d = data.designation.copy()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    var_mod = ['designation']\n",
    "    le = LabelEncoder()\n",
    "    for i in var_mod:\n",
    "        data[i] = le.fit_transform(data[i])\n",
    "    \n",
    "    \n",
    "    data = pd.concat([data,d],axis=1,ignore_index=False)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67bb6065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.resume_classifier(source_directory)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = r'E:\\DATA SETS\\Resumes'\n",
    "resume_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cf98c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd5dace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(resume_classifier(x), open('resume.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1276f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pickle.load(open('resume.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5dc86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534670da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34603cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a014c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
